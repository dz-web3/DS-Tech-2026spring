{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dz-web3/DS-Tech-2026spring/blob/main/Module8_LLM_Finetuning/LLM_Finetuning.ipynb)\n",
    "\n",
    "**Click the badge above to open this notebook in Google Colab!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: Fine-Tuning Large Language Models (LLMs)\n",
    "\n",
    "**Data Science for Business (Technical) \u2014 Spring 2026**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Explain** the difference between pre-training, fine-tuning, and prompting\n",
    "2. **Understand** when fine-tuning is the right choice vs. other approaches\n",
    "3. **Perform** hands-on fine-tuning of a Llama model using modern, efficient techniques\n",
    "4. **Evaluate** the business value and trade-offs of fine-tuning LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Business\n",
    "\n",
    "Large Language Models like GPT-4, Claude, and Llama are transforming how businesses operate. But **off-the-shelf models don't always fit your specific needs**. Fine-tuning allows you to:\n",
    "\n",
    "- \ud83c\udfaf **Customize** model behavior for your domain (legal, medical, customer service)\n",
    "- \ud83d\udcb0 **Reduce costs** by using smaller, specialized models instead of expensive large ones\n",
    "- \ud83d\udd12 **Maintain control** over your data and model behavior\n",
    "- \u26a1 **Improve performance** on specific tasks your business cares about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Google Colab Pro (Free for NYU Students)\n",
    "\n",
    "### \ud83c\udf93 NYU Students: Get Free Colab Pro!\n",
    "\n",
    "Google offers **free Colab Pro subscriptions** for students at U.S. higher education institutions.\n",
    "\n",
    "**To claim your free subscription:**\n",
    "\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Click on the gear icon (\u2699\ufe0f) \u2192 \"Colab Pro\"\n",
    "3. Select \"Colab Pro for Education\"\n",
    "4. Verify your student status using your NYU email\n",
    "5. You'll receive a **1-year free subscription** with more compute resources\n",
    "\n",
    "### \ud83d\udda5\ufe0f Enabling GPU for This Notebook\n",
    "\n",
    "Fine-tuning requires a GPU. Here's how to enable it:\n",
    "\n",
    "1. Go to **Runtime** \u2192 **Change runtime type**\n",
    "2. Set **Hardware accelerator** to **T4 GPU** (or A100 if available with Colab Pro)\n",
    "3. Click **Save**\n",
    "\n",
    "Run the cell below to verify GPU is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"\u2705 GPU is enabled: {gpu_name}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\u274c GPU is NOT enabled. Please go to Runtime \u2192 Change runtime type \u2192 Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The LLM Customization Spectrum\n",
    "\n",
    "Before diving into fine-tuning, let's understand where it fits in the broader landscape of LLM customization:\n",
    "\n",
    "| Approach | Effort | Data Needed | Use Case |\n",
    "|----------|--------|-------------|----------|\n",
    "| **Prompting** | Low | None | Quick tasks, general use |\n",
    "| **Few-shot Learning** | Low | 5-20 examples | Demonstrate desired format |\n",
    "| **RAG** (Retrieval) | Medium | Documents | Add knowledge, keep model current |\n",
    "| **Fine-tuning** | High | 100-10,000+ examples | Change model behavior/style |\n",
    "| **Pre-training** | Very High | Billions of tokens | Build from scratch (rarely needed) |\n",
    "\n",
    "### When Should You Fine-Tune?\n",
    "\n",
    "\u2705 **Fine-tune when you want to:**\n",
    "- Change the model's communication style consistently\n",
    "- Make the model follow specific formats/templates\n",
    "- Teach domain-specific terminology or behavior\n",
    "- Improve reliability on repetitive tasks\n",
    "\n",
    "\u274c **Don't fine-tune when you can:**\n",
    "- Solve the problem with better prompts\n",
    "- Use RAG to add relevant knowledge\n",
    "- Use few-shot examples in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Fine-Tuning: The Concept\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "**Pre-training** is like giving someone a general education:\n",
    "- The model learns from massive amounts of text (books, websites, code)\n",
    "- It learns language patterns, facts, and reasoning\n",
    "- This is expensive: millions of dollars, weeks of compute time\n",
    "- Done by companies like Meta (Llama), OpenAI (GPT), Google (Gemini)\n",
    "\n",
    "**Fine-tuning** is like specialized job training:\n",
    "- Start with a pre-trained model that already \"knows\" language\n",
    "- Train it on your specific examples to learn your style/domain\n",
    "- Much cheaper: can be done in minutes to hours on a single GPU\n",
    "- This is what **you** can do!\n",
    "\n",
    "### LoRA: Efficient Fine-Tuning\n",
    "\n",
    "Traditional fine-tuning updates **all** model parameters \u2014 expensive and slow.\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a clever technique that:\n",
    "- Freezes the original model weights\n",
    "- Adds small \"adapter\" layers that learn your specific task\n",
    "- Only trains ~1% of the parameters\n",
    "- Result: **Same quality, 10x less memory, 10x faster!**\n",
    "\n",
    "Think of it like this: instead of rewriting an entire textbook, you're adding sticky notes with your customizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Fine-Tuning Llama 3.2\n",
    "\n",
    "Now let's actually fine-tune a model! We'll use:\n",
    "\n",
    "- **Model**: Llama 3.2 1B (a capable but small model from Meta)\n",
    "- **Tool**: Unsloth (makes fine-tuning 2x faster and uses 70% less memory)\n",
    "- **Technique**: LoRA with 4-bit quantization\n",
    "- **Task**: Create a customer service chatbot\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "\n",
    "This will take 2-3 minutes. Go grab a coffee! \u2615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (optimized for Colab)\n",
    "!pip install unsloth\n",
    "# Install other dependencies\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import unsloth\n",
    "print(f\"\u2705 Unsloth version: {unsloth.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model\n",
    "\n",
    "We'll load Llama 3.2 1B in 4-bit quantization (uses ~1GB instead of ~4GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048  # Maximum context length\n",
    "dtype = None  # Auto-detect (float16 for T4)\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Model loaded successfully!\")\n",
    "print(f\"   Model: Llama 3.2 1B Instruct\")\n",
    "print(f\"   Parameters: ~1 billion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add LoRA Adapters\n",
    "\n",
    "Now we add the LoRA adapters \u2014 the small trainable layers that will learn our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank - higher = more capacity but more memory\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # 0 is optimized\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\u2705 LoRA adapters added!\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "print(f\"   Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare the Training Data\n",
    "\n",
    "We'll use a simple customer service dataset. In real scenarios, you'd use your own business data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample customer service training data\n",
    "training_data = [\n",
    "    {\"instruction\": \"What is your return policy?\", \"response\": \"Our return policy allows returns within 30 days of purchase with a valid receipt. Items must be in original condition with tags attached.\"},\n",
    "    {\"instruction\": \"How do I track my order?\", \"response\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking number from your shipping confirmation email.\"},\n",
    "    {\"instruction\": \"Do you offer international shipping?\", \"response\": \"Yes, we ship to over 50 countries worldwide. International shipping rates and delivery times vary by destination.\"},\n",
    "    {\"instruction\": \"How can I cancel my order?\", \"response\": \"To cancel an order, please contact us within 2 hours of placing it. Once an order has been processed for shipping, it cannot be cancelled.\"},\n",
    "    {\"instruction\": \"What payment methods do you accept?\", \"response\": \"We accept all major credit cards (Visa, Mastercard, Amex), PayPal, Apple Pay, and Google Pay.\"},\n",
    "    {\"instruction\": \"How do I reset my password?\", \"response\": \"Click 'Forgot Password' on the login page, enter your email, and we'll send you a reset link valid for 24 hours.\"},\n",
    "    {\"instruction\": \"Is my personal information secure?\", \"response\": \"Yes, we use industry-standard SSL encryption and never share your data with third parties. Your security is our priority.\"},\n",
    "    {\"instruction\": \"How do I apply a discount code?\", \"response\": \"Enter your discount code in the 'Promo Code' field at checkout and click 'Apply'. The discount will be reflected in your order total.\"},\n",
    "    {\"instruction\": \"What are your store hours?\", \"response\": \"Our online store is available 24/7. For physical locations, hours vary by store - please check our store locator for specific hours.\"},\n",
    "    {\"instruction\": \"How do I contact customer support?\", \"response\": \"You can reach us via live chat on our website, email at support@example.com, or call 1-800-EXAMPLE Monday-Friday 9am-6pm EST.\"},\n",
    "    {\"instruction\": \"Do you price match?\", \"response\": \"Yes, we offer price matching within 14 days of purchase if you find the same item at a lower price from an authorized retailer.\"},\n",
    "    {\"instruction\": \"How long does shipping take?\", \"response\": \"Standard shipping takes 5-7 business days. Express shipping (2-3 days) and overnight options are also available at checkout.\"},\n",
    "    {\"instruction\": \"Can I change my shipping address?\", \"response\": \"You can update your shipping address before the order ships by contacting customer support. Once shipped, address changes are not possible.\"},\n",
    "    {\"instruction\": \"Do you have a loyalty program?\", \"response\": \"Yes! Join our rewards program for free to earn points on every purchase, receive exclusive discounts, and get early access to sales.\"},\n",
    "    {\"instruction\": \"What if my item arrives damaged?\", \"response\": \"We're sorry to hear that! Please contact us within 48 hours with photos of the damage, and we'll send a replacement or issue a full refund.\"},\n",
    "]\n",
    "\n",
    "print(f\"\u2705 Training data prepared: {len(training_data)} examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Q: {training_data[0]['instruction']}\")\n",
    "print(f\"  A: {training_data[0]['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Format data for training\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the data into the chat template the model expects\"\"\"\n",
    "    text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful customer service assistant. Be friendly, professional, and concise.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['response']}<|eot_id|>\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "print(f\"\u2705 Dataset formatted!\")\n",
    "print(f\"\\nSample formatted prompt:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model! \ud83d\ude80\n",
    "\n",
    "This is where the magic happens. Training will take approximately **10-15 minutes** on a T4 GPU.\n",
    "\n",
    "Watch the loss decrease \u2014 that means the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,  # Quick training for demo\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\ud83d\ude80 Starting training...\")\n",
    "print(\"   This will take ~10-15 minutes on a T4 GPU\")\n",
    "print(\"   Watch the 'loss' value decrease - that means learning is happening!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(f\"\\n\u2705 Training complete!\")\n",
    "print(f\"   Total training time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"   Final loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Test the Fine-Tuned Model\n",
    "\n",
    "Let's see if our model learned to be a good customer service assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to inference mode (faster generation)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def ask_customer_service(question):\n",
    "    \"\"\"Ask our fine-tuned customer service model a question\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful customer service assistant. Be friendly, professional, and concise.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "print(\"\ud83e\udd16 Customer Service Bot Ready!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with questions from training data\n",
    "test_questions = [\n",
    "    \"What's your return policy?\",\n",
    "    \"How can I track my package?\",\n",
    "    \"Do you ship internationally?\",\n",
    "]\n",
    "\n",
    "print(\"\ud83d\udcdd Testing with training-similar questions:\\n\")\n",
    "for q in test_questions:\n",
    "    print(f\"Customer: {q}\")\n",
    "    print(f\"Bot: {ask_customer_service(q)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with NEW questions (not in training data)\n",
    "new_questions = [\n",
    "    \"Can I get a refund if I don't like the product?\",\n",
    "    \"What happens if my package is lost?\",\n",
    "    \"Do you have a size guide?\",\n",
    "]\n",
    "\n",
    "print(\"\ud83c\udd95 Testing with NEW questions (not in training data):\\n\")\n",
    "for q in new_questions:\n",
    "    print(f\"Customer: {q}\")\n",
    "    print(f\"Bot: {ask_customer_service(q)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Save the Model (Optional)\n",
    "\n",
    "If you want to use this model later, you can save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters (small, ~50MB)\n",
    "model.save_pretrained(\"customer_service_lora\")\n",
    "tokenizer.save_pretrained(\"customer_service_lora\")\n",
    "print(\"\u2705 Model saved to 'customer_service_lora' folder\")\n",
    "\n",
    "# Check the size\n",
    "!du -sh customer_service_lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Applications & Decision Framework\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "| Company | Application | Why Fine-Tuning? |\n",
    "|---------|-------------|------------------|\n",
    "| **Legal Tech** | Contract analysis | Domain-specific terminology |\n",
    "| **Healthcare** | Patient communication | Regulatory compliance, tone |\n",
    "| **E-commerce** | Customer service bots | Brand voice, product knowledge |\n",
    "| **Finance** | Report generation | Consistent formatting, compliance |\n",
    "| **Education** | Tutoring assistants | Teaching style, curriculum alignment |\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "**Fine-tuning costs:**\n",
    "- Compute: ~$1-10 for small models, ~$100-1000 for large models\n",
    "- Data preparation: Often the largest cost (human time to create/curate examples)\n",
    "- Iteration: Usually need 2-5 rounds to get it right\n",
    "\n",
    "**Fine-tuning benefits:**\n",
    "- 10-100x cheaper inference than prompting with examples\n",
    "- More consistent behavior\n",
    "- Faster response times (no need for long prompts)\n",
    "- Can use smaller, cheaper models\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Start with Prompting\n",
    "        \u2193\n",
    "Works well enough? \u2192 YES \u2192 Stop here! \ud83c\udf89\n",
    "        \u2193 NO\n",
    "Need external knowledge? \u2192 YES \u2192 Try RAG first\n",
    "        \u2193 NO\n",
    "Have 100+ good examples? \u2192 NO \u2192 Collect more data\n",
    "        \u2193 YES\n",
    "Fine-tune! \u2192 Evaluate \u2192 Iterate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Fine-tuning** adapts a pre-trained model to your specific needs\n",
    "2. **LoRA** makes fine-tuning efficient (train only ~1% of parameters)\n",
    "3. **Modern tools** (Unsloth) make it accessible on free hardware\n",
    "4. **Business value** comes from consistency, cost reduction, and customization\n",
    "\n",
    "### What We Did\n",
    "\n",
    "- \u2705 Loaded Llama 3.2 1B (a 1 billion parameter model)\n",
    "- \u2705 Added LoRA adapters for efficient training\n",
    "- \u2705 Fine-tuned on customer service data\n",
    "- \u2705 Tested the model on new questions\n",
    "- \u2705 Saved the model for future use\n",
    "\n",
    "### Next Steps for Your Career\n",
    "\n",
    "1. **Experiment**: Try fine-tuning with your own data\n",
    "2. **Explore**: Look into Hugging Face, OpenAI fine-tuning API\n",
    "3. **Stay current**: This field moves fast \u2014 follow AI news!\n",
    "\n",
    "---\n",
    "\n",
    "*Questions? Reach out during office hours or on the course forum.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Exercise (Optional)\n",
    "\n",
    "Try modifying the training data to create a different kind of assistant:\n",
    "\n",
    "1. **Tech Support Bot**: Answer questions about common tech issues\n",
    "2. **Restaurant Bot**: Handle reservations and menu questions\n",
    "3. **Fitness Coach**: Give workout advice and motivation\n",
    "\n",
    "Modify the `training_data` list with your own examples and re-run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom training data here!\n",
    "# my_training_data = [\n",
    "#     {\"instruction\": \"...\", \"response\": \"...\"},\n",
    "#     ...\n",
    "# ]"
   ]
  }
 ]
}