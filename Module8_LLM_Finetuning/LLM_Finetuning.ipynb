{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dz-web3/DS-Tech-2026spring/blob/main/Module8_LLM_Finetuning/LLM_Finetuning.ipynb)\n",
                "\n",
                "**Click the badge above to open this notebook in Google Colab!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 8: Fine-Tuning Large Language Models (LLMs)\n",
                "\n",
                "**Data Science for Business (Technical) ‚Äî Spring 2026**\n",
                "\n",
                "---\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this module, you will be able to:\n",
                "\n",
                "1. **Explain** the difference between pre-training, fine-tuning, and prompting\n",
                "2. **Understand** when fine-tuning is the right choice vs. other approaches\n",
                "3. **Perform** hands-on fine-tuning using Hugging Face and LoRA\n",
                "4. **Evaluate** the business value and trade-offs of fine-tuning LLMs\n",
                "\n",
                "---\n",
                "\n",
                "## Why This Matters for Business\n",
                "\n",
                "Large Language Models like GPT-4, Claude, and Llama are transforming how businesses operate. But **off-the-shelf models don't always fit your specific needs**. Fine-tuning allows you to:\n",
                "\n",
                "- üéØ **Customize** model behavior for your domain (legal, medical, customer service)\n",
                "- üí∞ **Reduce costs** by using smaller, specialized models instead of expensive large ones\n",
                "- üîê **Maintain control** over your data and model behavior\n",
                "- ‚ö° **Improve performance** on specific tasks your business cares about"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setting Up Google Colab Pro (Free for NYU Students)\n",
                "\n",
                "### üéì NYU Students: Get Free Colab Pro!\n",
                "\n",
                "Google offers **free Colab Pro subscriptions** for students at U.S. higher education institutions.\n",
                "\n",
                "**To claim your free subscription:**\n",
                "\n",
                "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
                "2. Click on the gear icon (‚öôÔ∏è) ‚Üí \"Colab Pro\"\n",
                "3. Select \"Colab Pro for Education\"\n",
                "4. Verify your student status using your NYU email\n",
                "5. You'll receive a **1-year free subscription** with more compute resources\n",
                "\n",
                "### üñ•Ô∏è Enabling GPU for This Notebook\n",
                "\n",
                "Fine-tuning requires a GPU. Here's how to enable it:\n",
                "\n",
                "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
                "2. Set **Hardware accelerator** to **T4 GPU** (or A100 if available with Colab Pro)\n",
                "3. Click **Save**\n",
                "\n",
                "Run the cell below to verify GPU is enabled:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if GPU is available\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    print(f\"‚úÖ GPU is enabled: {gpu_name}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ùå GPU is NOT enabled. Please go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The LLM Customization Spectrum\n",
                "\n",
                "Before diving into fine-tuning, let's understand where it fits in the broader landscape of LLM customization:\n",
                "\n",
                "| Approach | Effort | Data Needed | Use Case |\n",
                "|----------|--------|-------------|----------|\n",
                "| **Prompting** | Low | None | Quick tasks, general use |\n",
                "| **Few-shot Learning** | Low | 5-20 examples | Demonstrate desired format |\n",
                "| **RAG** (Retrieval) | Medium | Documents | Add knowledge, keep model current |\n",
                "| **Fine-tuning** | High | 100-10,000+ examples | Change model behavior/style |\n",
                "| **Pre-training** | Very High | Billions of tokens | Build from scratch (rarely needed) |\n",
                "\n",
                "### When Should You Fine-Tune?\n",
                "\n",
                "‚úÖ **Fine-tune when you want to:**\n",
                "- Change the model's communication style consistently\n",
                "- Make the model follow specific formats/templates\n",
                "- Teach domain-specific terminology or behavior\n",
                "- Improve reliability on repetitive tasks\n",
                "\n",
                "‚ùå **Don't fine-tune when you can:**\n",
                "- Solve the problem with better prompts\n",
                "- Use RAG to add relevant knowledge\n",
                "- Use few-shot examples in the prompt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Understanding Fine-Tuning: The Concept\n",
                "\n",
                "### Pre-training vs Fine-tuning\n",
                "\n",
                "**Pre-training** is like giving someone a general education:\n",
                "- The model learns from massive amounts of text (books, websites, code)\n",
                "- It learns language patterns, facts, and reasoning\n",
                "- This is expensive: millions of dollars, weeks of compute time\n",
                "- Done by companies like Meta (Llama), OpenAI (GPT), Google (Gemini)\n",
                "\n",
                "**Fine-tuning** is like specialized job training:\n",
                "- Start with a pre-trained model that already \"knows\" language\n",
                "- Train it on your specific examples to learn your style/domain\n",
                "- Much cheaper: can be done in minutes to hours on a single GPU\n",
                "- This is what **you** can do!\n",
                "\n",
                "### LoRA: Efficient Fine-Tuning\n",
                "\n",
                "Traditional fine-tuning updates **all** model parameters ‚Äî expensive and slow.\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)** is a clever technique that:\n",
                "- Freezes the original model weights\n",
                "- Adds small \"adapter\" layers that learn your specific task\n",
                "- Only trains ~1% of the parameters\n",
                "- Result: **Same quality, 10x less memory, 10x faster!**\n",
                "\n",
                "Think of it like this: instead of rewriting an entire textbook, you're adding sticky notes with your customizations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hands-On: Fine-Tuning with Hugging Face\n",
                "\n",
                "Now let's actually fine-tune a model! We'll use:\n",
                "\n",
                "- **Model**: GPT-2 Small (a classic, well-tested model perfect for learning)\n",
                "- **Library**: Hugging Face Transformers + PEFT (Parameter-Efficient Fine-Tuning)\n",
                "- **Technique**: LoRA adapters\n",
                "- **Task**: Create a customer service chatbot\n",
                "\n",
                "### Step 1: Install Dependencies\n",
                "\n",
                "This will take about 1 minute. ‚òï"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install Hugging Face libraries\n",
                "!pip install transformers datasets peft accelerate trl -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify installation\n",
                "import transformers\n",
                "import peft\n",
                "print(f\"‚úÖ Transformers version: {transformers.__version__}\")\n",
                "print(f\"‚úÖ PEFT version: {peft.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Load the Base Model\n",
                "\n",
                "We'll load GPT-2, a smaller model that's great for learning fine-tuning concepts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import torch\n",
                "\n",
                "# Model configuration\n",
                "model_name = \"gpt2\"  # 124M parameters - fast and efficient\n",
                "\n",
                "# Load the tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    device_map=\"auto\",\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Model loaded: {model_name}\")\n",
                "print(f\"   Parameters: ~124 million\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Add LoRA Adapters\n",
                "\n",
                "Now we add the LoRA adapters ‚Äî the small trainable layers that will learn our task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "\n",
                "# LoRA configuration\n",
                "lora_config = LoraConfig(\n",
                "    r=16,  # LoRA rank - higher = more capacity but more memory\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention layers\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.CAUSAL_LM\n",
                ")\n",
                "\n",
                "# Apply LoRA to the model\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "# Count trainable parameters\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "\n",
                "print(f\"‚úÖ LoRA adapters added!\")\n",
                "print(f\"   Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
                "print(f\"   Total parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Prepare the Training Data\n",
                "\n",
                "We'll use a simple customer service dataset. In real scenarios, you'd use your own business data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample customer service training data\n",
                "training_data = [\n",
                "    {\"instruction\": \"What is your return policy?\", \"response\": \"Our return policy allows returns within 30 days of purchase with a valid receipt. Items must be in original condition with tags attached.\"},\n",
                "    {\"instruction\": \"How do I track my order?\", \"response\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking number from your shipping confirmation email.\"},\n",
                "    {\"instruction\": \"Do you offer international shipping?\", \"response\": \"Yes, we ship to over 50 countries worldwide. International shipping rates and delivery times vary by destination.\"},\n",
                "    {\"instruction\": \"How can I cancel my order?\", \"response\": \"To cancel an order, please contact us within 2 hours of placing it. Once an order has been processed for shipping, it cannot be cancelled.\"},\n",
                "    {\"instruction\": \"What payment methods do you accept?\", \"response\": \"We accept all major credit cards (Visa, Mastercard, Amex), PayPal, Apple Pay, and Google Pay.\"},\n",
                "    {\"instruction\": \"How do I reset my password?\", \"response\": \"Click 'Forgot Password' on the login page, enter your email, and we'll send you a reset link valid for 24 hours.\"},\n",
                "    {\"instruction\": \"Is my personal information secure?\", \"response\": \"Yes, we use industry-standard SSL encryption and never share your data with third parties. Your security is our priority.\"},\n",
                "    {\"instruction\": \"How do I apply a discount code?\", \"response\": \"Enter your discount code in the 'Promo Code' field at checkout and click 'Apply'. The discount will be reflected in your order total.\"},\n",
                "    {\"instruction\": \"What are your store hours?\", \"response\": \"Our online store is available 24/7. For physical locations, hours vary by store - please check our store locator for specific hours.\"},\n",
                "    {\"instruction\": \"How do I contact customer support?\", \"response\": \"You can reach us via live chat on our website, email at support@example.com, or call 1-800-EXAMPLE Monday-Friday 9am-6pm EST.\"},\n",
                "    {\"instruction\": \"Do you price match?\", \"response\": \"Yes, we offer price matching within 14 days of purchase if you find the same item at a lower price from an authorized retailer.\"},\n",
                "    {\"instruction\": \"How long does shipping take?\", \"response\": \"Standard shipping takes 5-7 business days. Express shipping (2-3 days) and overnight options are also available at checkout.\"},\n",
                "    {\"instruction\": \"Can I change my shipping address?\", \"response\": \"You can update your shipping address before the order ships by contacting customer support. Once shipped, address changes are not possible.\"},\n",
                "    {\"instruction\": \"Do you have a loyalty program?\", \"response\": \"Yes! Join our rewards program for free to earn points on every purchase, receive exclusive discounts, and get early access to sales.\"},\n",
                "    {\"instruction\": \"What if my item arrives damaged?\", \"response\": \"We're sorry to hear that! Please contact us within 48 hours with photos of the damage, and we'll send a replacement or issue a full refund.\"},\n",
                "]\n",
                "\n",
                "print(f\"‚úÖ Training data prepared: {len(training_data)} examples\")\n",
                "print(f\"\\nExample:\")\n",
                "print(f\"  Q: {training_data[0]['instruction']}\")\n",
                "print(f\"  A: {training_data[0]['response']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "# Format data for training\n",
                "def format_prompt(example):\n",
                "    \"\"\"Format the data into a simple chat template\"\"\"\n",
                "    text = f\"\"\"### Customer Service Bot\n",
                "\n",
                "Customer: {example['instruction']}\n",
                "Agent: {example['response']}\n",
                "\"\"\"\n",
                "    return {\"text\": text}\n",
                "\n",
                "# Create dataset\n",
                "dataset = Dataset.from_list(training_data)\n",
                "dataset = dataset.map(format_prompt)\n",
                "\n",
                "print(f\"‚úÖ Dataset formatted!\")\n",
                "print(f\"\\nSample formatted prompt:\")\n",
                "print(dataset[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Train the Model! üöÄ\n",
                "\n",
                "This is where the magic happens. Training will take approximately **3-5 minutes** on a T4 GPU.\n",
                "\n",
                "Watch the loss decrease ‚Äî that means the model is learning!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Training configuration\n",
                "training_args = SFTConfig(\n",
                "    output_dir=\"./customer_service_model\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=2,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=5,\n",
                "    save_strategy=\"no\",\n",
                "    report_to=\"none\",\n",
                "    max_seq_length=256,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset,\n",
                ")\n",
                "\n",
                "print(\"üöÄ Starting training...\")\n",
                "print(\"   This will take ~3-5 minutes on a T4 GPU\")\n",
                "print(\"   Watch the 'loss' value decrease - that means learning is happening!\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training\n",
                "trainer.train()\n",
                "\n",
                "print(f\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Test the Fine-Tuned Model\n",
                "\n",
                "Let's see if our model learned to be a good customer service assistant!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set model to evaluation mode\n",
                "model.eval()\n",
                "\n",
                "def ask_customer_service(question):\n",
                "    \"\"\"Ask our fine-tuned customer service model a question\"\"\"\n",
                "    prompt = f\"\"\"### Customer Service Bot\n",
                "\n",
                "Customer: {question}\n",
                "Agent:\"\"\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=100,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract just the agent's response\n",
                "    response = response.split(\"Agent:\")[-1].strip()\n",
                "    # Stop at newline or end\n",
                "    response = response.split(\"\\n\")[0].strip()\n",
                "    return response\n",
                "\n",
                "print(\"ü§ñ Customer Service Bot Ready!\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with questions from training data\n",
                "test_questions = [\n",
                "    \"What's your return policy?\",\n",
                "    \"How can I track my package?\",\n",
                "    \"Do you ship internationally?\",\n",
                "]\n",
                "\n",
                "print(\"üìù Testing with training-similar questions:\\n\")\n",
                "for q in test_questions:\n",
                "    print(f\"Customer: {q}\")\n",
                "    print(f\"Bot: {ask_customer_service(q)}\")\n",
                "    print(\"-\" * 40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with NEW questions (not in training data)\n",
                "new_questions = [\n",
                "    \"Can I get a refund if I don't like the product?\",\n",
                "    \"What happens if my package is lost?\",\n",
                "    \"Do you have a size guide?\",\n",
                "]\n",
                "\n",
                "print(\"üÜï Testing with NEW questions (not in training data):\\n\")\n",
                "for q in new_questions:\n",
                "    print(f\"Customer: {q}\")\n",
                "    print(f\"Bot: {ask_customer_service(q)}\")\n",
                "    print(\"-\" * 40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 7: Save the Model (Optional)\n",
                "\n",
                "If you want to use this model later, you can save it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the LoRA adapters (small, ~10MB)\n",
                "model.save_pretrained(\"customer_service_lora\")\n",
                "tokenizer.save_pretrained(\"customer_service_lora\")\n",
                "print(\"‚úÖ Model saved to 'customer_service_lora' folder\")\n",
                "\n",
                "# Check the size\n",
                "!du -sh customer_service_lora/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Business Applications & Decision Framework\n",
                "\n",
                "### Real-World Use Cases\n",
                "\n",
                "| Company | Application | Why Fine-Tuning? |\n",
                "|---------|-------------|------------------|\n",
                "| **Legal Tech** | Contract analysis | Domain-specific terminology |\n",
                "| **Healthcare** | Patient communication | Regulatory compliance, tone |\n",
                "| **E-commerce** | Customer service bots | Brand voice, product knowledge |\n",
                "| **Finance** | Report generation | Consistent formatting, compliance |\n",
                "| **Education** | Tutoring assistants | Teaching style, curriculum alignment |\n",
                "\n",
                "### Cost-Benefit Analysis\n",
                "\n",
                "**Fine-tuning costs:**\n",
                "- Compute: ~$1-10 for small models, ~$100-1000 for large models\n",
                "- Data preparation: Often the largest cost (human time to create/curate examples)\n",
                "- Iteration: Usually need 2-5 rounds to get it right\n",
                "\n",
                "**Fine-tuning benefits:**\n",
                "- 10-100x cheaper inference than prompting with examples\n",
                "- More consistent behavior\n",
                "- Faster response times (no need for long prompts)\n",
                "- Can use smaller, cheaper models\n",
                "\n",
                "### Decision Framework\n",
                "\n",
                "```\n",
                "Start with Prompting\n",
                "        ‚Üì\n",
                "Works well enough? ‚Üí YES ‚Üí Stop here! üéâ\n",
                "        ‚Üì NO\n",
                "Need external knowledge? ‚Üí YES ‚Üí Try RAG first\n",
                "        ‚Üì NO\n",
                "Have 100+ good examples? ‚Üí NO ‚Üí Collect more data\n",
                "        ‚Üì YES\n",
                "Fine-tune! ‚Üí Evaluate ‚Üí Iterate\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary & Key Takeaways\n",
                "\n",
                "### What We Learned\n",
                "\n",
                "1. **Fine-tuning** adapts a pre-trained model to your specific needs\n",
                "2. **LoRA** makes fine-tuning efficient (train only ~1% of parameters)\n",
                "3. **Hugging Face** provides easy-to-use tools for fine-tuning\n",
                "4. **Business value** comes from consistency, cost reduction, and customization\n",
                "\n",
                "### What We Did\n",
                "\n",
                "- ‚úÖ Loaded GPT-2 (a 124 million parameter model)\n",
                "- ‚úÖ Added LoRA adapters for efficient training\n",
                "- ‚úÖ Fine-tuned on customer service data\n",
                "- ‚úÖ Tested the model on new questions\n",
                "- ‚úÖ Saved the model for future use\n",
                "\n",
                "### Next Steps for Your Career\n",
                "\n",
                "1. **Experiment**: Try fine-tuning with your own data\n",
                "2. **Explore**: Look into Hugging Face Hub, OpenAI fine-tuning API\n",
                "3. **Stay current**: This field moves fast ‚Äî follow AI news!\n",
                "\n",
                "---\n",
                "\n",
                "*Questions? Reach out during office hours or on the course forum.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Required Tasks\n",
                "\n",
                "Complete the following two task notebooks to practice what you've learned:\n",
                "\n",
                "### Task 1: Sentiment Fine-Tuning\n",
                "**Notebook**: `Task1_Sentiment_Finetuning.ipynb`\n",
                "\n",
                "[![Open Task 1 in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dz-web3/DS-Tech-2026spring/blob/main/Module8_LLM_Finetuning/Task1_Sentiment_Finetuning.ipynb)\n",
                "\n",
                "In this task, you will:\n",
                "- Fine-tune a model on product reviews\n",
                "- Add your own training examples\n",
                "- Test the model on custom text\n",
                "\n",
                "---\n",
                "\n",
                "### Task 2: Prompting vs Fine-Tuning\n",
                "**Notebook**: `Task2_Prompting_vs_Finetuning.ipynb`\n",
                "\n",
                "[![Open Task 2 in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dz-web3/DS-Tech-2026spring/blob/main/Module8_LLM_Finetuning/Task2_Prompting_vs_Finetuning.ipynb)\n",
                "\n",
                "In this task, you will:\n",
                "- Compare zero-shot vs few-shot prompting\n",
                "- Improve prompts for better responses\n",
                "- Create a decision framework for business scenarios"
            ]
        }
    ]
}